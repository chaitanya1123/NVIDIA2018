{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA GTC 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What?\n",
    "## Where?\n",
    "Insert temperature conditions here\n",
    "## When?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It all started with a workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert Andrew Ng´s comment on posing problems and solving them with deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workshops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep Learning for Finance Trading Strategy\n",
    "Good code, but not linked to the use cases they presented. I had a bit of a hard time following the trading jargon, but what I wanted out of this session was details on the analytical setup. I had a certain project from my company in mind that I could try deep learning on. But the intro slide from the instructor kind of crushed my ideas already in the beginning. The quote in questions was about applying deep learning to simple problems with large datasets. If a domain expert can phrase the problem in question as a simple equation, and given abundance of data, than the problem can be tackled with deep learning. However, if the problem is complex and you cannot formulate a simple answer to it, and few data is available, you don´t have to bother with deep learning. \n",
    "As it usually goes, I got most out of the workshop through follow up discussions. Me and me companion next to me had a very vibrant discussion with the instructors regarding technical details of the setup of the network, and how you could transfer the learnings to other problems. We are still awaiting some follow up mails the instructors promissed to send on the questions we posed.\n",
    "* Anomaly Detection for Variational Autoencoders\n",
    "Image recogncition but \n",
    "* Genomics: Using Deep Learning to Accelerate the Idenfitication of Genetic Variants\n",
    "* The Necessity of Explainability Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keynotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jensen Huang (Founder & CEO @NVIDIA)\n",
    "Insert photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Jurgen Schmidthuber (Scientific Director @IDSIA)\n",
    "\n",
    "Great perspective and cross cut through historical events (biological evolution) that lead to current tech revolution. Distopian thoughts about the speed of tech evolution, and AI evolving and expanding into the universe.\n",
    "How to chose the next task if you don´t have one on the belt: it should be the one that breaks the capacity but not the skillset of the current most advanced problem solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pannel discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to Create Talented Teams and Deliver Successful AI Solutions\n",
    "Staring:\n",
    "* Miachael Holmstroem (Managing Director @Academy.se)\n",
    "* Will Ramey (Senior Director Gloval Head of Developer Programs @NVIDIA)\n",
    "* Timo Stich (Senior Research Scientist @ZEISS) >>> Love the guys' name!\n",
    "* Ulli Waltinger (CT Research in Digitalization and Automation @Siemens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Talks\n",
    "* That crazy guy from PlantVision\n",
    "Explain why I have a healthy doubt about his phenotyping ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CEO @PureStorage\n",
    "\n",
    "Ice ages of AI since 1960s (third peak happening in this decade). In the previous AI warmings, would prefer to lose code rather than data cause most time and effort was put into producing code. Nowdays would prefer to lose code, cause you´re mostly reusing anyway, but investing a lot of time into collecting and storing data. Plus relying on data for AI is crutial, cause without history there´s no prediction.\n",
    "Ecosystem around ML code is creating a technical debt of AI (need a lot of power and storage space for the tiny bit of the data flow that is the machine learning analytics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expo and demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi ride back to the airport"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
